{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Viewer\n",
    "\n",
    "This notebook demonstrates how to read and query Delta Lake tables stored in MinIO (S3-compatible storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Spark Session with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Create Spark session with Delta Lake configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeViewer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Delta table path\n",
    "delta_table_path = \"s3a://delta-lake/tables/customers\"\n",
    "\n",
    "# Read Delta table\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Show schema\n",
    "print(\"Table Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show data\n",
    "print(\"\\nTable Data:\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total records\n",
    "total_count = df.count()\n",
    "print(f\"Total records: {total_count}\")\n",
    "\n",
    "# Show statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by name\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example: Filter customers whose name contains 'John'\n",
    "filtered_df = df.filter(col(\"name\").contains(\"John\"))\n",
    "filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delta Lake Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Delta Lake table history\n",
    "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "print(\"Delta Table History:\")\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a specific version (time travel)\n",
    "# Example: Read version 0\n",
    "version_0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "print(\"Data at version 0:\")\n",
    "version_0_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register table as SQL temporary view\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        email,\n",
    "        DATE(created_at) as signup_date\n",
    "    FROM customers\n",
    "    ORDER BY created_at DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas_df = df.toPandas()\n",
    "print(pandas_df.head())\n",
    "\n",
    "# Example: Plot count by operation type (if available)\n",
    "if 'operation' in pandas_df.columns:\n",
    "    operation_counts = pandas_df['operation'].value_counts()\n",
    "    operation_counts.plot(kind='bar', title='Operations Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (optional)\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
