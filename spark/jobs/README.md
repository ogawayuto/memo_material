# Spark Jobs - Kafka Avro Streaming

PySparkã‚’ä½¿ã£ãŸKafka Avroã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã®ã‚¸ãƒ§ãƒ–é›†ã€‚

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
spark/jobs/
â”œâ”€â”€ kafka_to_deltalake.py           # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ãƒ§ãƒ–
â”œâ”€â”€ avro_deserializer.py            # æ±ç”¨Avroãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”œâ”€â”€ example_usage.py                # Avroãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ã®ä½¿ç”¨ä¾‹
â”œâ”€â”€ requirements.txt                # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ KAFKA_AVRO_STREAMING_GUIDE.md   # è©³ç´°å®Ÿè£…ã‚¬ã‚¤ãƒ‰ â­
â”œâ”€â”€ AVRO_DESERIALIZER_README.md     # Avroãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â””â”€â”€ README.md                       # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã®èµ·å‹•

```bash
# ã‚¸ãƒ§ãƒ–ã‚’é–‹å§‹
./scripts/manage-streaming-job.sh start

# ã‚¸ãƒ§ãƒ–ã‚’åœæ­¢
./scripts/manage-streaming-job.sh stop

# ã‚¸ãƒ§ãƒ–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
./scripts/manage-streaming-job.sh status
```

### 2. ãƒ­ã‚°ã®ç¢ºèª

```bash
# å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºï¼ˆæˆåŠŸç‡ãªã©ï¼‰
./scripts/check-spark-logs.sh summary

# ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ­ã‚°ã‚’è¡¨ç¤º
./scripts/check-spark-logs.sh deserialize

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿è¡¨ç¤º
./scripts/check-spark-logs.sh errors

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
./scripts/check-spark-logs.sh live

# ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
./scripts/check-spark-logs.sh help
```

**å®Ÿè¡Œä¾‹**ï¼š
```bash
$ ./scripts/check-spark-logs.sh summary
=== å‡¦ç†ã‚µãƒãƒªãƒ¼ ===

ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆ:
[DESERIALIZE BATCH] Processed 1 records: 1 success, 0 errors
[DESERIALIZE BATCH] Processed 1 records: 1 success, 0 errors
...

æˆåŠŸ/ã‚¨ãƒ©ãƒ¼é›†è¨ˆ:
ç·å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: 5
æˆåŠŸ: 5
ã‚¨ãƒ©ãƒ¼: 0
æˆåŠŸç‡: 100.00%
```

### 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥

```bash
# PostgreSQLã«æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ 
docker exec postgres psql -U postgres -d sourcedb -c \
  "INSERT INTO customers (name, email) VALUES ('Test User', 'test@example.com');"

# 5-10ç§’å¾Œã«Delta Lakeã«æ›¸ãè¾¼ã¾ã‚Œã‚‹
```

## ğŸ“š ä¸»è¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

### [KAFKA_AVRO_STREAMING_GUIDE.md](KAFKA_AVRO_STREAMING_GUIDE.md) â­ **å¿…èª­**

è©³ç´°ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’å«ã¿ã¾ã™ï¼š

- **å‡¦ç†ãƒ•ãƒ­ãƒ¼**: ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã¨å„ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°
- **é‡è¦ãªå®Ÿè£…ãƒã‚¤ãƒ³ãƒˆ**: AvroDeserializerã®æ­£ã—ã„ä½¿ã„æ–¹
- **æ³¨æ„ç‚¹**: ã‚ˆãã‚ã‚‹é–“é•ã„ã¨ãã®å¯¾å‡¦æ³•
- **ãƒ­ã‚°ã®ç¢ºèªæ–¹æ³•**: ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•

### [AVRO_DESERIALIZER_README.md](AVRO_DESERIALIZER_README.md)

æ±ç”¨Avroãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼š

- ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæŸ”è»ŸãªMapTypeã€æ˜ç¤ºçš„ãªã‚¹ã‚­ãƒ¼ãƒã€è‡ªå‹•æ¨è«–ï¼‰
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®æ¯”è¼ƒ

## ğŸ”‘ é‡è¦ãƒã‚¤ãƒ³ãƒˆ

### AvroDeserializerã®æ­£ã—ã„åˆæœŸåŒ–

**âœ… æ­£è§£**ï¼š
```python
deserializer = AvroDeserializer(
    schema_registry_client=client,
    schema_str=None  # â† ã“ã‚ŒãŒé‡è¦ï¼è‡ªå‹•ã§Schema IDã‹ã‚‰å–å¾—
)
```

**âŒ é–“é•ã„**ï¼š
```python
deserializer = AvroDeserializer(
    schema_registry_client=client,
    schema_str=schema.schema_str  # â† ã“ã‚Œã ã¨ã‚¨ãƒ©ãƒ¼
)
```

### Confluent Wire Format

```
Byte 0:     Magic Byte (0x00)
Bytes 1-4:  Schema ID (big-endian)
Bytes 5+:   Avro payload
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### NULLãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæ›¸ãè¾¼ã¾ã‚Œã‚‹

1. **ãƒ­ã‚°ã‚’ç¢ºèª**ï¼š
   ```bash
   ./scripts/check-spark-logs.sh errors
   ```

2. **ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ­ã‚°ã‚’ç¢ºèª**ï¼š
   ```bash
   ./scripts/check-spark-logs.sh deserialize -n 50
   ```

3. **å‡¦ç†çµ±è¨ˆã‚’ç¢ºèª**ï¼š
   ```bash
   ./scripts/check-spark-logs.sh summary
   ```

### PyArrowã‚¨ãƒ©ãƒ¼

```bash
# requirements.txtã«pyarrow>=14.0.0ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
cat spark/jobs/requirements.txt | grep pyarrow

# Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å†ãƒ“ãƒ«ãƒ‰
docker-compose build spark-master spark-worker
docker-compose up -d --force-recreate spark-master spark-worker
```

### Schema Registryæ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# Schema RegistryãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª
docker ps | grep schema-registry

# Schema Registryã«æ¥ç¶šã§ãã‚‹ã‹ç¢ºèª
curl http://localhost:8085/subjects

# Sparkå†…éƒ¨ã‹ã‚‰æ¥ç¶šç¢ºèª
docker exec spark-master curl http://schema-registry:8081/subjects
```

## ğŸ“Š å‡¦ç†ç¢ºèª

### Delta Lakeãƒ†ãƒ¼ãƒ–ãƒ«ã®å†…å®¹ç¢ºèª

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

df = spark.read.format("delta").load("s3a://delta-lake/tables/customers")
df.orderBy("kafka_timestamp", ascending=False).show(10, truncate=False)
```

## ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

ã‚ˆã‚Šè©³ç´°ãªãƒ­ã‚°ãŒå¿…è¦ãªå ´åˆã¯ã€`kafka_to_deltalake.py`ã®`parse_confluent_avro`é–¢æ•°å†…ã«ãƒ­ã‚°ã‚’è¿½åŠ ï¼š

```python
import sys

def parse_confluent_avro(binary_data, topic="cdc.public.customers"):
    # è©³ç´°ãƒ­ã‚°
    sys.stderr.write(f"[DEBUG] Input: {len(binary_data)} bytes\n")
    sys.stderr.write(f"[DEBUG] First 20 bytes: {binary_data[:20].hex()}\n")
    sys.stderr.flush()

    # ... å‡¦ç†ç¶šè¡Œ
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### ç¾åœ¨ã®è¨­å®š

- **ãƒãƒƒãƒé–“éš”**: 5ç§’
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: S3 (MinIO)
- **Executor**: 2 cores, 2GB RAM

### ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹

```python
# ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
spark.conf.set("spark.sql.streaming.kafka.maxOffsetsPerTrigger", 1000)

# ãƒ¡ãƒ¢ãƒªå¢—é‡
spark.conf.set("spark.executor.memory", "4g")
```

## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯

- [Confluent Kafka Python Docs](https://docs.confluent.io/kafka-clients/python/current/overview.html)
- [Schema Registry API](https://docs.confluent.io/platform/current/schema-registry/develop/api.html)
- [PySpark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Delta Lake Documentation](https://delta.io/)

## âš¡ Tips

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€

```bash
# Executor logs
/opt/spark/work/app-*/0/stderr
/opt/spark/work/app-*/0/stdout

# æœ€æ–°ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³IDã‚’ç¢ºèª
docker logs spark-master 2>&1 | grep "Registered app" | tail -1
```

### ã‚ˆãä½¿ã†ã‚³ãƒãƒ³ãƒ‰

```bash
# Kafkaãƒˆãƒ”ãƒƒã‚¯ç¢ºèª
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Schema Registry subjectsç¢ºèª
curl http://localhost:8085/subjects | jq

# MinIO (Delta Lake) ç¢ºèª
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:9001 ã«ã‚¢ã‚¯ã‚»ã‚¹
# ãƒ¦ãƒ¼ã‚¶ãƒ¼å: minioadmin / ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰: minioadmin
```

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License
